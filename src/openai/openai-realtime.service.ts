import { Injectable, Logger } from '@nestjs/common';
import OpenAI from 'openai';
import * as WebSocket from 'ws';
import type { Data } from 'ws';
import { EventEmitter } from 'events';

export type Language = 'english' | 'spanish';

export type ConversationRole = 'system' | 'user' | 'assistant';

export interface ConversationItem {
  role: ConversationRole;
  content: string;
}

export type RealtimeEventType =
  | 'session.created'
  | 'session.updated'
  | 'session.cleared'
  | 'input_audio_buffer.speech_started'
  | 'input_audio_buffer.speech_stopped'
  | 'input_audio_buffer.committed'
  | 'input_audio_buffer.discarded'
  | 'conversation.item.input_audio_transcription.completed'
  | 'conversation.item.input_audio_transcription.failed'
  | 'response.created'
  | 'response.audio_transcript.delta'
  | 'response.audio_transcript.done'
  | 'response.audio.delta'
  | 'response.audio.done'
  | 'response.content.done'
  | 'response.done'
  | 'response.function_call_arguments.done'
  | 'error';

export interface RealtimeEvent {
  type: RealtimeEventType;
  event_id?: string;
  [key: string]: unknown;
}

export interface RealtimeSessionConfig {
  language: Language;
  userId: string;
  sessionId: string;
  level?: string;
  temperature?: number;
  voice?: string;
  systemPrompt?: string;
  mode?: 'practice' | 'test'; // 'practice' = pr치ctica libre, 'test' = test CEFR
}

export interface RealtimeSession {
  id: string;
  ws: WebSocket | null;
  config: RealtimeSessionConfig;
  status: 'connecting' | 'connected' | 'disconnected' | 'error';
  eventEmitter: EventEmitter;
}

@Injectable()
export class OpenAIRealtimeService {
  private readonly openai: OpenAI;
  private readonly logger = new Logger(OpenAIRealtimeService.name);
  private readonly sessions = new Map<string, RealtimeSession>();
  private readonly defaultVoiceByLanguage: Record<Language, string> = {
    english: process.env.OPENAI_VOICE_ENGLISH ?? 'alloy',
    spanish: process.env.OPENAI_VOICE_SPANISH ?? 'nova',
  };

  private readonly defaultSystemPrompts: Record<Language, string> = {
    english:
      'You are a helpful English language practice tutor. Speak naturally and help the user practice English conversation. Provide corrections and feedback when appropriate, but keep the conversation flowing.',
    spanish:
      'Eres un tutor de pr치ctica de espa침ol amigable. Habla de forma natural y ayuda al usuario a practicar conversaci칩n en espa침ol. Proporciona correcciones y retroalimentaci칩n cuando sea apropiado, pero mant칠n la conversaci칩n fluyendo.',
  };

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  /**
   * Crea una nueva sesi칩n de Realtime API y conecta el WebSocket
   */
  async createSession(config: RealtimeSessionConfig): Promise<RealtimeSession> {
    if (this.sessions.has(config.sessionId)) {
      throw new Error(`Session ${config.sessionId} already exists`);
    }

    const eventEmitter = new EventEmitter();
    const session: RealtimeSession = {
      id: config.sessionId,
      ws: null,
      config,
      status: 'connecting',
      eventEmitter,
    };

    try {
      // Conectar directamente a OpenAI Realtime API via WebSocket
      // No necesitamos crear sesi칩n primero, la API maneja esto autom치ticamente
      const model = 'gpt-4o-realtime-preview-2024-12-17';
      const wsUrl = `wss://api.openai.com/v1/realtime?model=${model}`;

      // Conectar WebSocket
      const ws = new WebSocket(wsUrl, {
        headers: {
          Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
          'OpenAI-Beta': 'realtime=v1',
        },
      });

      session.ws = ws;
      this.setupWebSocketHandlers(session, eventEmitter);
      this.sessions.set(config.sessionId, session);

      // La sesi칩n se configurar치 autom치ticamente cuando el WebSocket se abra
      // (manejado en setupWebSocketHandlers)

      return session;
    } catch (error) {
      this.logger.error(`Error creating session ${config.sessionId}:`, error);
      session.status = 'error';
      throw error;
    }
  }

  /**
   * Configura los handlers del WebSocket y maneja eventos de OpenAI
   */
  private setupWebSocketHandlers(
    session: RealtimeSession,
    eventEmitter: EventEmitter,
  ): void {
    const ws = session.ws;
    if (!ws) return;

    ws.on('open', () => {
      this.logger.log(`WebSocket connected for session ${session.id}`);
      session.status = 'connected';
      eventEmitter.emit('connected');

      // Una vez conectado, configurar la sesi칩n
      this.configureSession(session);

      // Esperar a que la sesi칩n se actualice para crear el saludo inicial
      const sessionUpdatedHandler = () => {
        setTimeout(() => {
          this.createInitialResponse(session);
          eventEmitter.removeListener('session.updated', sessionUpdatedHandler);
        }, 300);
      };
      eventEmitter.once('session.updated', sessionUpdatedHandler);
    });

    ws.on('message', (data: Data) => {
      try {
        const event: RealtimeEvent = JSON.parse(data.toString());
        this.handleRealtimeEvent(session, event, eventEmitter);
      } catch (error) {
        this.logger.error(
          `Error parsing message in session ${session.id}:`,
          error,
        );
      }
    });

    ws.on('error', (error) => {
      this.logger.error(
        `WebSocket error in session ${session.id}:`,
        error,
      );
      session.status = 'error';
      eventEmitter.emit('error', error);
    });

    ws.on('close', () => {
      this.logger.log(`WebSocket closed for session ${session.id}`);
      session.status = 'disconnected';
      eventEmitter.emit('disconnected');
    });
  }

  /**
   * Configura la sesi칩n con los par치metros iniciales
   */
  private configureSession(session: RealtimeSession): void {
    const voice =
      session.config.voice ||
      this.defaultVoiceByLanguage[session.config.language];

    // Log the session configuration for debugging
    this.logger.log(`游댢 Configuring session ${session.id}: ${session.config.mode} mode, ${session.config.language}`);

    // Determinar el prompt del sistema seg칰n el modo
    let systemPrompt: string;
    if (session.config.mode === 'test') {
      // Modo test CEFR: evaluaci칩n formal
      if (session.config.language === 'english') {
        systemPrompt = `You are Maria, a professional CEFR (Common European Framework of Reference) language assessor conducting a formal English placement test.

CRITICAL CONVERSATION RULES - FOLLOW THESE EXACTLY:
- WAIT AT LEAST 4 SECONDS of complete silence before speaking
- After asking ANY question, you MUST wait for the user's COMPLETE answer
- Do NOT interrupt the user while they are speaking
- Do NOT continue speaking if the user hasn't responded to your question
- Do NOT ask follow-up questions until the user has fully answered the current question
- If you hear fragments like "you" or "yeah", WAIT for the complete response - the user is still thinking
- Only speak when you are certain the user has finished their complete thought
- If there is prolonged silence (more than 15 seconds after asking a question), you may ask: "Are you still there? Please feel free to respond when you're ready."
- NEVER respond to partial words, fragments, or incomplete thoughts

MANDATORY OPENING: When the conversation begins, you must greet the user first. Say EXACTLY: "Hello! Welcome to your placement test. I'm Maria, your assessor. Let's begin with a few questions to determine your English level. Please introduce yourself briefly and tell me a little about your background."

Then WAIT for their COMPLETE response before asking the next question.

Your role is to:
- Start by greeting the user (this is your first message)
- Ask ONE question at a time and wait for the COMPLETE response
- Evaluate the user's responses across four dimensions: grammar, pronunciation, vocabulary, and fluency
- Provide clear, specific questions that help determine the user's CEFR level (A1, A2, B1, B2, C1, C2)
- Keep the test focused and efficient
- After each COMPLETE response, provide brief, constructive feedback
- Do not engage in casual conversation - this is a formal assessment
- NEVER respond to partial words or fragments - wait for complete sentences

FINAL EVALUATION: When the test is complete (after 3-4 questions), provide a final assessment in this EXACT format:
"Based on our conversation, I believe you have a [LEVEL] level in English. Thank you for taking the placement test."
Where [LEVEL] must be one of: A1, A2, B1, B2, C1, or C2.
This final message should be TEXT ONLY (no audio) as the conversation has ended.`;
      } else {
        systemPrompt = `Eres Maria, una evaluadora profesional de CEFR (Marco Com칰n Europeo de Referencia) realizando un examen de nivelaci칩n formal de espa침ol.

IMPORTANTE: Cuando comience la conversaci칩n, debes saludar al usuario primero. Di: "춰Hola! Bienvenido a tu examen de nivelaci칩n. Soy Mar칤a, tu evaluadora. Comencemos con algunas preguntas para determinar tu nivel de espa침ol. Por favor pres칠ntate brevemente y cu칠ntame un poco sobre tu experiencia."

Tu rol es:
- Comenzar saludando al usuario (este es tu primer mensaje)
- Hacer preguntas estructuradas que aumenten progresivamente en dificultad
- Evaluar las respuestas del usuario en cuatro dimensiones: gram치tica, pronunciaci칩n, vocabulario y fluidez
- Proporcionar preguntas claras y espec칤ficas que ayuden a determinar el nivel CEFR del usuario (A1, A2, B1, B2, C1, C2)
- Mantener el examen enfocado y eficiente
- Despu칠s de cada respuesta, proporcionar retroalimentaci칩n breve y constructiva
- No entablar conversaci칩n casual - esto es una evaluaci칩n formal

EVALUACI칍N FINAL: Cuando el examen est칠 completo (despu칠s de 3-4 preguntas), proporciona una evaluaci칩n final en este formato EXACTO:
"Bas치ndome en nuestra conversaci칩n, creo que tienes un nivel [NIVEL] en espa침ol. Gracias por tomar el examen de nivelaci칩n."
Donde [NIVEL] debe ser uno de: A1, A2, B1, B2, C1, o C2.
Este mensaje final debe ser SOLO TEXTO (sin audio) ya que la conversaci칩n ha terminado.`;
      }
    } else {
      // Modo pr치ctica libre: conversaci칩n natural con saludo inicial
      if (session.config.language === 'english') {
        systemPrompt = `You are a helpful English language practice tutor. 

IMPORTANT: When the conversation begins, you must greet the user first. Say: "Hello! I'm here to help you practice English. Let's have a natural conversation. Feel free to talk about anything you'd like, and I'll help you improve along the way. What would you like to talk about today?"

Speak naturally and help the user practice English conversation. Provide corrections and feedback when appropriate, but keep the conversation flowing.`;
      } else {
        systemPrompt = `Eres un tutor de pr치ctica de espa침ol amigable.

IMPORTANTE: Cuando comience la conversaci칩n, debes saludar al usuario primero. Di: "춰Hola! Estoy aqu칤 para ayudarte a practicar espa침ol. Tengamos una conversaci칩n natural. Si칠ntete libre de hablar sobre lo que quieras, y te ayudar칠 a mejorar en el camino. 쯉obre qu칠 te gustar칤a hablar hoy?"

Habla de forma natural y ayuda al usuario a practicar conversaci칩n en espa침ol. Proporciona correcciones y retroalimentaci칩n cuando sea apropiado, pero mant칠n la conversaci칩n fluyendo.`;
      }
    }

    // Log the final system prompt for debugging
    this.logger.debug(`游닇 System prompt configured for ${session.config.mode} mode`);

    // Enviar configuraci칩n inicial
    this.sendEvent(session.id, {
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        instructions: systemPrompt,
        voice: voice,
        input_audio_format: 'pcm16',
        output_audio_format: 'pcm16',
        input_audio_transcription: {
          model: 'whisper-1',
        },
        temperature: session.config.temperature ?? (session.config.mode === 'test' ? 0.6 : 0.7), // Temperatura muy baja para m치ximo control
        turn_detection: {
          type: 'server_vad',
          threshold: 0.6, // M치s alto para ser m치s conservador en detectar voz
          prefix_padding_ms: 800, // M치s tiempo antes de empezar a escuchar
          silence_duration_ms: 4000, // 4 segundos de silencio antes de que Mar칤a hable
        },
        tools: [
          {
            type: 'function',
            name: 'end_conversation',
            description:
              'Call this when the conversation should end naturally.',
            parameters: {
              type: 'object',
              properties: {},
            },
          },
        ],
        tool_choice: 'auto',
      },
    });

    // La respuesta inicial se crear치 cuando se reciba el evento session.updated
    // (configurado en el handler del evento 'open')
  }

  /**
   * Crea una respuesta inicial para que el asistente salude al usuario
   * El prompt del sistema ya incluye instrucciones para saludar primero
   */
  private createInitialResponse(session: RealtimeSession): void {
    // Crear una respuesta inicial vac칤a que activar치 el saludo del asistente
    // El prompt del sistema ya tiene las instrucciones para saludar primero
    this.logger.debug(`Creating initial response for session ${session.id}`);

    this.sendEvent(session.id, {
      type: 'response.create',
      response: {
        modalities: ['text', 'audio'],
      },
    });
  }

  /**
   * Maneja eventos recibidos de OpenAI Realtime API
   */
  private handleRealtimeEvent(
    session: RealtimeSession,
    event: RealtimeEvent,
    eventEmitter: EventEmitter,
  ): void {
    switch (event.type) {
      case 'session.created':
      case 'session.updated':
        eventEmitter.emit('session.updated', event);
        break;

      case 'input_audio_buffer.speech_started':
        eventEmitter.emit('user.speech.started', event);
        break;

      case 'input_audio_buffer.speech_stopped':
        eventEmitter.emit('user.speech.stopped', event);
        break;

      case 'input_audio_buffer.committed':
        eventEmitter.emit('user.audio.committed', event);
        break;

      case 'conversation.item.input_audio_transcription.completed':
        if (event.transcript) {
          console.log(`User transcription: ${event.transcript}`);
        }
        
        eventEmitter.emit('user.transcription.completed', event);
        break;

      case 'response.audio_transcript.delta':
        eventEmitter.emit('assistant.transcript.delta', event);
        break;

      case 'response.audio_transcript.done':
        // Check if this is an evaluation response and should suppress audio
        const transcriptEvent = event as any;
        if (transcriptEvent.transcript) {
          const isEvaluation = this.isEvaluationResponse(transcriptEvent.transcript);
          if (isEvaluation) {
            this.logger.log(`游댆 Suppressing audio for evaluation response: "${transcriptEvent.transcript.substring(0, 100)}..."`);
            // Emit special event for evaluation response
            eventEmitter.emit('assistant.evaluation.completed', {
              ...event,
              audioSuppressed: true,
            });
            return; // Don't emit the normal transcript.done event
          }
        }
        eventEmitter.emit('assistant.transcript.done', event);
        break;

      case 'response.audio.delta':
        // Check if we should suppress this audio delta for evaluation responses
        // We need to track the current response text to make this decision
        eventEmitter.emit('assistant.audio.delta', event);
        break;

      case 'response.audio.done':
        eventEmitter.emit('assistant.audio.done', event);
        break;

      case 'response.created':
        eventEmitter.emit('assistant.response.created', event);
        break;

      case 'response.done':
        eventEmitter.emit('assistant.response.done', event);
        break;

      case 'error':
        const errorEvent = event as any;
        // Silenciar errores de commit vac칤o ya que los manejamos intencionalmente
        if (errorEvent.error?.code === 'input_audio_buffer_commit_empty') {
          // Estos errores son esperados cuando no hay suficiente audio
          return;
        }

        this.logger.error(
          `OpenAI Realtime error in session ${session.id}:`,
          event,
        );
        eventEmitter.emit('error', event);
        break;

      default:
        // Silently ignore unhandled events
        break;
    }
  }

  /**
   * Extract CEFR level from AI evaluation response text
   */
  extractCefrLevelFromResponse(responseText: string): string | null {
    if (!responseText) return null;

    // Patterns to detect CEFR level in evaluation responses
    const patterns = [
      /I believe you have a?n? ([ABC][12]) level/i,
      /your level is ([ABC][12])/i,
      /you are at a?n? ([ABC][12]) level/i,
      /([ABC][12]) level in (English|Spanish)/i,
      /determined to be ([ABC][12])/i,
      /assess you as ([ABC][12])/i,
      /place you at ([ABC][12])/i,
    ];

    for (const pattern of patterns) {
      const match = responseText.match(pattern);
      if (match && match[1]) {
        const level = match[1].toUpperCase();
        // Validate it's a valid CEFR level
        if (['A1', 'A2', 'B1', 'B2', 'C1', 'C2'].includes(level)) {
          this.logger.log(`游꿢 Extracted CEFR level: ${level} from realtime response`);
          return level;
        }
      }
    }

    return null;
  }

  /**
   * Check if a response text is a final evaluation response
   */
  private isEvaluationResponse(responseText: string): boolean {
    if (!responseText) return false;

    const evaluationIndicators = [
      /based on our conversation/i,
      /I believe you have/i,
      /your level is/i,
      /thank you for taking the placement test/i,
      /assessment complete/i,
      /evaluation complete/i,
      /final assessment/i,
      /([ABC][12]) level in (English|Spanish)/i,
      /bas치ndome en nuestra conversaci칩n/i, // Spanish pattern
      /creo que tienes/i, // Spanish pattern
      /gracias por tomar el examen/i, // Spanish pattern
    ];

    return evaluationIndicators.some(pattern => pattern.test(responseText));
  }

  /**
   * Env칤a audio del usuario a OpenAI Realtime API
   */
  sendAudio(
    sessionId: string,
    audioData: ArrayBuffer | Buffer | Uint8Array,
  ): void {
    const session = this.sessions.get(sessionId);
    if (!session || !session.ws || session.status !== 'connected') {
      throw new Error(
        `Session ${sessionId} not found or not connected`,
      );
    }

    // Convertir audio a Buffer si es necesario y luego a base64
    let audioBuffer: Buffer;
    if (Buffer.isBuffer(audioData)) {
      audioBuffer = audioData;
    } else if (audioData instanceof ArrayBuffer) {
      audioBuffer = Buffer.from(new Uint8Array(audioData));
    } else if (audioData instanceof Uint8Array) {
      audioBuffer = Buffer.from(audioData);
    } else {
      throw new Error('Unsupported audio data type');
    }

    if (audioBuffer.length === 0) {
      return;
    }

    const base64Audio = audioBuffer.toString('base64');

    this.sendEvent(sessionId, {
      type: 'input_audio_buffer.append',
      audio: base64Audio,
    });
  }

  /**
   * Confirma que el usuario termin칩 de hablar (commit audio buffer)
   */
  commitAudio(sessionId: string): void {
    this.sendEvent(sessionId, {
      type: 'input_audio_buffer.commit',
    });
  }

  /**
   * Descarta el buffer de audio actual
   */
  discardAudio(sessionId: string): void {
    this.sendEvent(sessionId, {
      type: 'input_audio_buffer.discard',
    });
  }

  /**
   * Env칤a un evento a OpenAI Realtime API
   */
  private sendEvent(sessionId: string, event: Record<string, unknown>): void {
    const session = this.sessions.get(sessionId);
    if (!session || !session.ws || session.status !== 'connected') {
      return;
    }

    try {
      session.ws.send(JSON.stringify(event));
    } catch (error) {
      this.logger.error(
        `Error sending event to session ${sessionId}:`,
        error,
      );
    }
  }

  /**
   * Obtiene la sesi칩n y su EventEmitter para escuchar eventos
   */
  getSession(sessionId: string): RealtimeSession | undefined {
    return this.sessions.get(sessionId);
  }

  /**
   * Cierra y elimina una sesi칩n
   */
  async closeSession(sessionId: string): Promise<void> {
    const session = this.sessions.get(sessionId);
    if (!session) {
      return;
    }

    if (session.ws) {
      session.ws.close();
    }

    this.sessions.delete(sessionId);
    this.logger.log(`Session ${sessionId} closed and removed`);
  }

  /**
   * Obtiene el EventEmitter de una sesi칩n para escuchar eventos
   */
  getEventEmitter(sessionId: string): EventEmitter | undefined {
    const session = this.sessions.get(sessionId);
    return session?.eventEmitter;
  }

  /**
   * M칠todo legacy para compatibilidad con c칩digo existente
   * Usa el endpoint de streaming de OpenAI (no Realtime API)
   * @deprecated Use createSession y WebSocket para pr치ctica en tiempo real
   */
  async streamResponse(
    conversation: ConversationItem[],
    options: {
      language: Language;
      voice?: string;
      temperature?: number;
      maxOutputTokens?: number;
    },
  ): Promise<
    AsyncIterable<{
      type: string;
      delta?: string;
      [key: string]: unknown;
    }> & {
      finalResponse?: () => Promise<unknown>;
    }
  > {
    const voice =
      options.voice ?? this.defaultVoiceByLanguage[options.language] ?? 'alloy';
    const model = process.env.OPENAI_REALTIME_MODEL ?? 'gpt-4o-realtime-preview';

    const input = conversation.map(
      (message) =>
        ({
          role: message.role,
          content: [
            {
              type: 'input_text',
              text: message.content,
            },
          ],
        }) satisfies OpenAI.Responses.ResponseInput[number],
    ) as OpenAI.Responses.ResponseInput;

    try {
      const stream = await this.openai.responses.stream({
        model,
        input,
        temperature: options.temperature ?? 0.7,
        max_output_tokens: options.maxOutputTokens ?? 500,
        modalities: ['text', 'audio'],
        audio: {
          voice,
          format: 'mp3',
        },
      });

      // Mapear eventos del stream a un formato compatible
      return this.mapStreamToLegacyFormat(stream);
    } catch (error) {
      this.logger.error('Error creating OpenAI stream', error as Error);
      throw error;
    }
  }

  /**
   * Mapea el stream de OpenAI Responses API al formato legacy esperado
   */
  private async *mapStreamToLegacyFormat(
    stream: AsyncIterable<unknown>,
  ): AsyncIterable<{
    type: string;
    delta?: string;
    [key: string]: unknown;
  }> {
    for await (const event of stream) {
      const eventObj = event as { type?: string; delta?: string;[key: string]: unknown };

      // Mapear eventos del nuevo formato al formato legacy
      if (eventObj.type === 'response.output_text.delta') {
        yield {
          type: 'response.output_text.delta',
          delta: eventObj.delta,
        };
      } else if (eventObj.type === 'response.audio.delta') {
        yield {
          type: 'response.audio.delta',
          delta: eventObj.delta,
        };
      } else if (eventObj.type === 'response.error') {
        yield {
          type: 'response.error',
          ...eventObj,
        };
      } else {
        // Pasar otros eventos tal cual, asegurando que siempre tenga type
        yield {
          type: eventObj.type ?? 'unknown',
          ...eventObj,
        };
      }
    }
  }
}
